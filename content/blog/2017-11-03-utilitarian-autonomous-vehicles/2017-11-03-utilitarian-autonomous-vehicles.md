---
title: "Anyone Else But Me: Utilitarian Autonomous Vehicles"
date: "2017-11-03"
path: "/posts/utilitarian-autonomous-vehicles"
---
A couple of months ago while I was still in school, I had to write a paper discussing an ethical conflict. As someone who works in technology and is constantly exposed to the ever-growing conversation of artificial intelligence, the discussion of a machine's moral agency felt like the most compelling abstract to sink my teeth into.

I decided to focus on the "modern trolley problem" -- where an autonomous vehicle has to decide between protecting it's occupants even if it means swerving into a crowd of pedestrians, or make the utilitarian calculation to cause minimal loss of life even if that means killing the people inside it.

![This, is called a "trolley", not "shopping cart". But we're not here to discuss this type of trolley problem.](image1.jpg)

I wish I spent more time writing an analysis into social dilemmas with more depth and how it might help us predict what the future holds for utilitarian autonomous vehicles beyond the results of a single social psychological survey, but I was unfortunately working against a strict deadline and word limit.

Without spoiling the entire post in the introduction, here it is:

-- 

Autonomous vehicles today promise a future of roads with increased traffic efficiency, pollution reduction, and elimination of up to 90% of traffic accidents. These promised improvements have potential to save a lot of people once it hits critical mass, but however doesn't guarantee the elimination of traffic accidents completely and still has the potential to kill, hence creating a need for new types of road safety regulations -- with a greater emphasis on situations where harm cannot be avoided. 

Some call it the "modern trolley problem" -- in which an autonomous vehicle has to decide between two scenarios -- protecting it's occupants even if it means swerving into a crowd of pedestrians, or make the utilitarian calculation to cause minimal loss of life even if that means killing the people inside it. As self-driving technology and it's algorithms mature, discussions have started to form around these cognitive systems and their moral agency: should these vehicles be considered moral agents and be held liable for their decisions and actions? Should car manufacturers augment their training algorithms in making utilitarian moral choices? In the following paragraphs, we will define self-driving technology's role as moral agents, illustrate how comfortable consumers are with utilitarian autonomous vehicles, and finally discuss the role of policy and car makers and how they can seek the balance between cost, safety, and convenience as we usher in this new epoch of human-computer interaction.

Today, most of the skepticism around Artificial Intelligence as moral agents are born out of the assumption that humans are moral decision-makers and that artificial intelligence should be able to replace our capacities to function as moral agents as well. In his book "Machines of Loving Grace", John Markoff writes, "The best way to answer the hard questions about control in a world full of smart machines is by understanding the values of those who are actually building these systems". He cannot be any more right. Since artificial intelligence today is still largely built by human designers, the values car manufacturers uphold will then translate directly to the moral status ascribed to them by their creators. Car-makers are then faced with this question which changes how we think about this ethical challenge all together: who would buy a car programmed to sacrifice their owner?

It's evidently obvious that the answer to our trolley problem question therein lies in the opinion of the public, as their social acceptance will determine the pace of adoption of self-driving cars, which is highly likely to save more lives overall. A set of surveys from a group of researchers from the Toulouse School of Economics in France has revealed surprising conclusions on utilitarian autonomous vehicles: when harm is unavoidable, people praise autonomous systems to make the utilitarian moral decision of self-sacrifice in order to pursue the greater good and save the lives of other individuals on the road. However, Bonnefon and co revealed that this conclusion has it's limitations: "[Participants] were not as confident that autonomous vehicles would be programmed that way in reality and for a good reason: they actually wished others to cruise in utilitarian autonomous vehicles, more than they wanted to buy utilitarian autonomous vehicles themselves". This paradox represents the classic social dilemma where people agree that utiliarian autonomous vehicles are best for the greater good but no one is willing to adopt it themselves in their own self-interest. This looks like a challenge on the surface, but can provide car manufacturers as well as regulators a hint on what is to come: while self-interest might work against utilitarian autonomous vehicles in the short term, social norms will eventually favour once they achieve widespread adoption.

Since self-driving technology is now at it's infancy, it is not in any car manufacturers' best interest to reveal their answer to the trolley problem as neither the public or regulators have made up their mind on the right approach. Mercedes, however, through a slip of the tongue from their manager of driver-assistance systems has revealed that the company will make it the duty of the vehicle to protect it's occupants over pedestrians. Mercedes' plan, if true, exposes a reality that is Mercedes' business of selling cars, and they merely are catering to the current needs of the public -- who now favour their own safety as a priority when faced with a thorny situation. Until the market for autonomous vehicles matures and people settle into social norms, Mercedes' strategy isn't completely preposterous to push for larger prevalence of self-driving cars in the short term, leading to safer roads overall. Car manufacturers can then adapt according to societal and regulatory requirements and push out software updates to their self-driving software, just as Tesla Motors does to provide their customers various self-driving features.

Regulators can cope with this new technology by making the right investments to support accelerating the increase of number of self-driving cars on roads. This also means putting a greater focus into building infrastructure instead of increasing the time and cost required to deploy driverless cars. The city of Columbus in Ohio have already started deploying roadside sensors and cameras for self-driving cars to operate easily with highway infrastucture, ensuring greater safety of vehicles on the road. These types of investments help incentivise the public to have a positive opinion on autonomous vehicles by showing them how it's larger prevalence could not only help them save liters of wasted petrol, hours lost stuck in traffic, as well as lives.

When it's all said and done, autonomous vehicles are going to become a commonplace in the future and has great promise in improving productivity of people, ensuring better road safety, and reducing harm to the environment. For car makers and regulators to do anything but encourage and accelerate the initial adoption of these fully-autonomous vehicles will be unethical -- as such a promise of social and environmental improvement to the world stands to benefit us all.
